/**
 * SpeechTest.jsx â€” NeuroAid V4
 * ================================
 * Uses the Web Speech API (SpeechRecognition) for REAL transcription,
 * plus the Web Audio API for real pause detection and amplitude analysis.
 *
 * Accurate metrics extracted:
 *   - WPM:                   real word count / real elapsed time
 *   - Transcription accuracy: word-level diff vs passage (Levenshtein)
 *   - Pause ratio:            actual silence frames / total frames (Web Audio)
 *   - Speech variability:     std-dev of per-sentence WPM (real segments)
 *   - Start delay:            time from start button to first speech event
 *   - Word finding failures:  filler words ("uh", "um", "er", "hmm") count
 *   - Repetitions:            consecutive duplicate word pairs
 *   - Completion ratio:       transcript length vs passage length
 *
 * Fallback: if SpeechRecognition is unavailable (Firefox/Safari), the test
 * falls back to MediaRecorder + timer-based estimation with real pause detection.
 */

import { useState, useRef, useEffect, useCallback } from "react";
import { T } from "../utils/theme";
import { DarkCard, Btn } from "./RiskDashboard";
import { useAssessment } from "../context/AssessmentContext";

// â”€â”€ Passages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
const BUILTIN_PASSAGES = [
  "The sun rises slowly over the mountains each morning, casting golden light across the valley. Birds begin their song as the world awakens. The river flows steadily, carrying the day forward with quiet patience and grace.",
  "A gentle breeze moved through the tall grass near the old farmhouse. Children laughed as they chased butterflies across the meadow. The afternoon light was warm and golden, and everything felt peaceful and unhurried.",
  "The library was quiet except for the soft turning of pages. Dust floated in the beams of sunlight that came through the tall windows. She had been reading for hours and still did not want to stop.",
  "Every morning he walked the same path along the river. He noticed the small changes â€” a new bird, a fallen branch, the way the water moved after rain. These details gave his days a steady rhythm.",
  "The old clock on the wall ticked slowly through the evening. Outside, rain fell against the windows in steady waves. She wrapped a blanket around herself and watched the fire burn low in the hearth.",
  "Spring arrived quietly that year, with cool mornings and longer afternoons. The garden began to fill with color â€” yellow, white, and soft purple blooms. The air smelled of earth and something new beginning.",
];

async function loadPassages() {
  try {
    const token = sessionStorage.getItem("neuroaid_token");
    if (token) {
      const res = await fetch("/api/content", { headers: { Authorization: `Bearer ${token}` } });
      if (res.ok) {
        const data = await res.json();
        const custom = (data.passages || []).map(p => p.text);
        return [...BUILTIN_PASSAGES, ...custom];
      }
    }
  } catch (e) {}
  return BUILTIN_PASSAGES;
}

// â”€â”€ NLP helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

const FILLERS = new Set(["uh", "um", "er", "hmm", "ah", "like", "you know", "basically"]);

function normalizeWord(w) {
  return w.toLowerCase().replace(/[^a-z']/g, "");
}

function tokenize(text) {
  return text.split(/\s+/).map(normalizeWord).filter(Boolean);
}

// Word-level accuracy: longest common subsequence ratio
function lcsLength(a, b) {
  const m = a.length, n = b.length;
  if (m === 0 || n === 0) return 0;
  const dp = Array.from({ length: m + 1 }, () => new Array(n + 1).fill(0));
  for (let i = 1; i <= m; i++)
    for (let j = 1; j <= n; j++)
      dp[i][j] = a[i - 1] === b[j - 1] ? dp[i - 1][j - 1] + 1 : Math.max(dp[i - 1][j], dp[i][j - 1]);
  return dp[m][n];
}

function wordAccuracy(transcript, passage) {
  const tw = tokenize(transcript);
  const pw = tokenize(passage);
  if (pw.length === 0) return 100;
  const lcs = lcsLength(tw, pw);
  return Math.round((lcs / pw.length) * 100);
}

function countFillers(transcript) {
  return tokenize(transcript).filter(w => FILLERS.has(w)).length;
}

function countRepetitions(transcript) {
  const words = tokenize(transcript);
  let reps = 0;
  for (let i = 1; i < words.length; i++)
    if (words[i] === words[i - 1] && words[i].length > 2) reps++;
  return reps;
}

function computeSpeedVariability(segmentWpms) {
  if (segmentWpms.length < 2) return 0;
  const mean = segmentWpms.reduce((a, b) => a + b, 0) / segmentWpms.length;
  const variance = segmentWpms.reduce((s, v) => s + (v - mean) ** 2, 0) / segmentWpms.length;
  return Math.round(Math.sqrt(variance));
}

// Split transcript into roughly equal sentence segments and compute per-segment WPM
function segmentWpms(transcript, totalSec) {
  const sentences = transcript.split(/[.!?,;]+/).filter(s => s.trim().length > 3);
  if (sentences.length < 2) return [Math.round((tokenize(transcript).length / totalSec) * 60)];
  const totalWords = tokenize(transcript).length;
  const secPerWord = totalSec / Math.max(totalWords, 1);
  return sentences.map(s => {
    const wc = tokenize(s).length;
    const dur = wc * secPerWord;
    return dur > 0 ? Math.round((wc / dur) * 60) : 0;
  }).filter(v => v > 0);
}

// â”€â”€ Word highlight helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

function getWordMatchMap(transcript, passage) {
  const tw = tokenize(transcript);
  const pw = tokenize(passage);
  const passageWords = passage.split(/\s+/);
  const matched = new Set();
  let ti = 0;
  for (let pi = 0; pi < pw.length && ti < tw.length; pi++) {
    if (pw[pi] === tw[ti]) { matched.add(pi); ti++; }
  }
  return { passageWords, matched };
}

const fmt = s => `${String(Math.floor(s / 60)).padStart(2, "0")}:${String(s % 60).padStart(2, "0")}`;

const LIME = "#C8F135";

// â”€â”€ Main component â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

export default function SpeechTest({ setPage }) {
  const { setSpeechData } = useAssessment();

  const [passage,       setPassage]       = useState(BUILTIN_PASSAGES[0]);
  const [phase,         setPhase]         = useState("idle");
  const [timer,         setTimer]         = useState(0);
  const [restartCount,  setRestart]       = useState(0);
  const [result,        setResult]        = useState(null);
  const [micError,      setMicError]      = useState("");
  const [transcript,    setTranscript]    = useState("");
  const [interimText,   setInterimText]   = useState("");
  const [usingSpeechAPI,setUsingSpeechAPI]= useState(false);
  const [pauseRatio,    setPauseRatio]    = useState(0);
  const [wordHighlight, setWordHighlight] = useState(null); // {passageWords, matched}
  const [liveWpm,       setLiveWpm]       = useState(0);

  // Refs
  const recognitionRef  = useRef(null);
  const mediaRef        = useRef(null);
  const chunksRef       = useRef([]);
  const intervalRef     = useRef(null);
  const startTs         = useRef(null);
  const firstSpeechTs   = useRef(null);
  const audioCtxRef     = useRef(null);
  const analyserRef     = useRef(null);
  const rafRef          = useRef(null);
  const silentFrames    = useRef(0);
  const totalFrames     = useRef(0);
  const finalTranscript = useRef("");
  const segmentTimes    = useRef([]); // timestamps of each recognition result

  const hasSpeechAPI = typeof window !== "undefined" &&
    (window.SpeechRecognition || window.webkitSpeechRecognition);

  useEffect(() => {
    loadPassages().then(passages => {
      setPassage(passages[Math.floor(Math.random() * passages.length)]);
    });
  }, []);

  // â”€â”€ Real-time pause detection via Web Audio â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  function startPauseDetection(stream) {
    try {
      const ctx = new (window.AudioContext || window.webkitAudioContext)();
      const src = ctx.createMediaStreamSource(stream);
      const an  = ctx.createAnalyser();
      an.fftSize = 512;
      src.connect(an);
      audioCtxRef.current = ctx;
      analyserRef.current = an;
      silentFrames.current = 0;
      totalFrames.current  = 0;

      const data = new Uint8Array(an.frequencyBinCount);
      const SILENCE_THRESHOLD = 8; // RMS below this = silence

      function tick() {
        an.getByteTimeDomainData(data);
        // Compute RMS amplitude
        let sum = 0;
        for (let i = 0; i < data.length; i++) {
          const v = (data[i] - 128) / 128;
          sum += v * v;
        }
        const rms = Math.sqrt(sum / data.length) * 100;

        totalFrames.current++;
        if (rms < SILENCE_THRESHOLD) silentFrames.current++;

        // Update live pause ratio
        if (totalFrames.current % 30 === 0) {
          const pr = silentFrames.current / totalFrames.current;
          setPauseRatio(Math.round(pr * 100));
        }

        rafRef.current = requestAnimationFrame(tick);
      }
      rafRef.current = requestAnimationFrame(tick);
    } catch (e) {}
  }

  function stopPauseDetection() {
    cancelAnimationFrame(rafRef.current);
    if (audioCtxRef.current) {
      audioCtxRef.current.close();
      audioCtxRef.current = null;
    }
  }

  // â”€â”€ Speech Recognition (Chrome/Edge) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  function startWithSpeechAPI(stream) {
    setUsingSpeechAPI(true);
    const SR = window.SpeechRecognition || window.webkitSpeechRecognition;
    const rec = new SR();
    rec.continuous     = true;
    rec.interimResults = true;
    rec.lang           = "en-US";
    rec.maxAlternatives= 1;

    finalTranscript.current = "";
    segmentTimes.current    = [];

    rec.onresult = (e) => {
      let interim = "";
      for (let i = e.resultIndex; i < e.results.length; i++) {
        const t = e.results[i][0].transcript;
        if (e.results[i].isFinal) {
          finalTranscript.current += t + " ";
          segmentTimes.current.push({ text: t, ts: Date.now() });
          if (!firstSpeechTs.current) firstSpeechTs.current = Date.now();

          // Live word count â†’ WPM
          const elapsed = (Date.now() - startTs.current) / 1000;
          const wc      = tokenize(finalTranscript.current).length;
          if (elapsed > 2) setLiveWpm(Math.round((wc / elapsed) * 60));

          // Live highlight
          setWordHighlight(getWordMatchMap(finalTranscript.current, passage));
        } else {
          interim = t;
          if (!firstSpeechTs.current && t.trim()) firstSpeechTs.current = Date.now();
        }
      }
      setTranscript(finalTranscript.current);
      setInterimText(interim);
    };

    rec.onerror = (e) => {
      if (e.error === "not-allowed") {
        setMicError("Microphone access denied. Please allow microphone permission.");
        setPhase("error");
      }
    };

    recognitionRef.current = rec;
    rec.start();
    startPauseDetection(stream);
  }

  // â”€â”€ MediaRecorder fallback â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  function startWithMediaRecorder(stream) {
    setUsingSpeechAPI(false);
    const mimeType = ["audio/webm;codecs=opus","audio/webm","audio/ogg","audio/mp4"]
      .find(t => MediaRecorder.isTypeSupported(t)) || "";
    const mr = new MediaRecorder(stream, mimeType ? { mimeType } : {});
    mediaRef.current  = mr;
    chunksRef.current = [];
    mr.ondataavailable = e => { if (e.data.size > 0) chunksRef.current.push(e.data); };
    mr.start(500);
    startPauseDetection(stream);
    finalTranscript.current = "";
  }

  // â”€â”€ Start recording â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  async function startRec() {
    setMicError("");
    setTranscript("");
    setInterimText("");
    setWordHighlight(null);
    setLiveWpm(0);
    setPauseRatio(0);
    finalTranscript.current = "";

    try {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      startTs.current      = Date.now();
      firstSpeechTs.current = null;

      if (hasSpeechAPI) {
        startWithSpeechAPI(stream);
      } else {
        startWithMediaRecorder(stream);
      }

      setPhase("recording");
      setTimer(0);
      intervalRef.current = setInterval(() => setTimer(t => t + 1), 1000);

    } catch (err) {
      const msg = err.name === "NotAllowedError"
        ? "Microphone access denied. Please allow microphone permission in your browser."
        : `Microphone error: ${err.message}`;
      setMicError(msg);
    }
  }

  // â”€â”€ Stop and analyse â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  async function stopRec(isRestart = false) {
    clearInterval(intervalRef.current);
    stopPauseDetection();

    // Stop speech recognition
    if (recognitionRef.current) {
      try { recognitionRef.current.stop(); } catch (e) {}
      recognitionRef.current = null;
    }

    // Stop media recorder
    const mr = mediaRef.current;
    if (mr && mr.state !== "inactive") {
      mr.stop();
      mr.stream?.getTracks().forEach(t => t.stop());
      mediaRef.current = null;
    }

    if (isRestart) {
      setPhase("idle");
      setTimer(0);
      setTranscript("");
      setInterimText("");
      setWordHighlight(null);
      setLiveWpm(0);
      setRestart(r => r + 1);
      return;
    }

    setPhase("processing");
    await new Promise(r => setTimeout(r, 400));

    const totalSec    = Math.max(timer, 1);
    const fullText    = finalTranscript.current.trim();
    const passageWC   = tokenize(passage).length;

    // â”€â”€ Compute all accurate metrics â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    // WPM: from real transcribed word count
    const transcribedWC = tokenize(fullText).length;
    // Guard: require at least 5s of audio to avoid absurd WPM from accidental clicks
    const minRecordSec = 5;
    const effectiveSec = Math.max(totalSec, minRecordSec);
    const wpm           = usingSpeechAPI
      ? Math.round((transcribedWC / effectiveSec) * 60)
      : Math.round((passageWC    / effectiveSec) * 60);  // fallback: assume passage read

    // Word accuracy (0â€“100): how many passage words appeared in transcript
    const accuracy = usingSpeechAPI ? wordAccuracy(fullText, passage) : null;

    // Completion ratio: what fraction of passage was spoken
    const compRatio = usingSpeechAPI
      ? Math.min(transcribedWC / passageWC, 1.0)
      : Math.min(effectiveSec / 35, 1.0);  // 35s â‰ˆ average read time

    // Real pause ratio from Web Audio
    const finalPauseR = totalFrames.current > 0
      ? parseFloat((silentFrames.current / totalFrames.current).toFixed(3))
      : 0.15;

    // Speed variability from per-sentence WPMs
    const segWpms = usingSpeechAPI
      ? segmentWpms(fullText, totalSec)
      : [wpm, wpm * 0.9, wpm * 1.1]; // approximate without transcription
    const speedVar = computeSpeedVariability(segWpms);

    // Fillers and repetitions (only with transcription)
    const fillerCount = usingSpeechAPI ? countFillers(fullText) : 0;
    const repCount    = usingSpeechAPI ? countRepetitions(fullText) : 0;

    // Start delay (seconds from button press to first speech)
    const startDelay = firstSpeechTs.current
      ? parseFloat(((firstSpeechTs.current - startTs.current) / 1000).toFixed(2))
      : 0.8;

    // Pause ratio: blend audio + speech gaps for max accuracy
    const speechGapPenalty = (fillerCount * 0.02) + (repCount * 0.015);
    const pauseRatioFinal  = Math.min(finalPauseR + speechGapPenalty, 0.95);

    // Get audio B64 for backend (optional)
    let audioB64 = null;
    try {
      if (chunksRef.current.length > 0) {
        const blob = new Blob(chunksRef.current, { type: "audio/webm" });
        audioB64 = await new Promise(res => {
          const r = new FileReader();
          r.onloadend = () => res(r.result.split(",")[1] || null);
          r.onerror   = () => res(null);
          r.readAsDataURL(blob);
        });
      }
    } catch (e) {}

    const payload = {
      audio_b64:                audioB64,
      wpm:                      wpm,
      speed_deviation:          speedVar,
      speech_speed_variability: speedVar,
      pause_ratio:              parseFloat(pauseRatioFinal.toFixed(3)),
      completion_ratio:         parseFloat(compRatio.toFixed(3)),
      restart_count:            restartCount,
      speech_start_delay:       startDelay,
    };

    setSpeechData(payload);
    setResult({
      wpm, speedVar, compRatio, startDelay, pauseRatio: pauseRatioFinal,
      accuracy, fillerCount, repCount, totalSec,
      transcribedWC, usedSpeechAPI: usingSpeechAPI,
    });
    setPhase("done");
  }

  // Cleanup
  useEffect(() => () => {
    clearInterval(intervalRef.current);
    stopPauseDetection();
    if (recognitionRef.current) { try { recognitionRef.current.stop(); } catch (e) {} }
    if (mediaRef.current && mediaRef.current.state !== "inactive") {
      mediaRef.current.stop();
      mediaRef.current.stream?.getTracks().forEach(t => t.stop());
    }
  }, []);

  // â”€â”€ Render â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  const passageWords = passage.split(/\s+/);

  return (
    <div>
      <button
        onClick={() => { stopRec(true).catch(() => {}); setPage("assessments"); }}
        style={{ background: "none", border: "none", color: T.creamFaint, cursor: "pointer",
          fontFamily: "'DM Sans',sans-serif", fontSize: 13, marginBottom: 24 }}>
        â† Back
      </button>

      <div style={{ display: "flex", alignItems: "flex-start", justifyContent: "space-between", marginBottom: 6 }}>
        <h1 style={{ fontFamily: "'Instrument Serif',serif", fontSize: 36, color: T.cream, letterSpacing: -1 }}>
          Speech Analysis
        </h1>
        <div style={{ fontSize: 11, color: T.creamFaint, background: hasSpeechAPI ? "rgba(74,222,128,0.1)" : "rgba(245,158,11,0.1)",
          border: `1px solid ${hasSpeechAPI ? "rgba(74,222,128,0.25)" : "rgba(245,158,11,0.25)"}`,
          borderRadius: 20, padding: "5px 12px", marginTop: 4 }}>
          {hasSpeechAPI ? "âœ“ AI Transcription Active" : "âš  Audio-only Mode (Chrome recommended)"}
        </div>
      </div>
      <p style={{ color: T.creamFaint, fontSize: 14, marginBottom: 28 }}>
        Read the passage aloud clearly. Real-time transcription measures accuracy, fluency, and pause patterns.
      </p>

      {micError && (
        <div style={{ background: "rgba(232,64,64,0.1)", border: "1px solid rgba(232,64,64,0.3)",
          borderRadius: 12, padding: "14px 18px", marginBottom: 20, color: T.red, fontSize: 13, lineHeight: 1.6 }}>
          âš ï¸ {micError}
        </div>
      )}

      <div style={{ display: "grid", gridTemplateColumns: "1fr 1fr", gap: 20 }}>

        {/* â”€â”€ Left col â”€â”€ */}
        <div style={{ display: "flex", flexDirection: "column", gap: 16 }}>

          {/* Passage card with word highlighting */}
          <DarkCard style={{ padding: 28 }} hover={false}>
            <div style={{ fontSize: 11, color: T.creamFaint, letterSpacing: 1,
              textTransform: "uppercase", marginBottom: 14 }}>
              Reading Passage
              <span style={{ color: "#555", fontWeight: 400 }}> ({passageWords.length} words)</span>
            </div>
            <div style={{
              borderLeft: `3px solid ${phase === "recording" ? T.red : "rgba(255,255,255,0.1)"}`,
              paddingLeft: 16, transition: "border-color 0.3s",
              fontSize: 15, lineHeight: 2.0,
            }}>
              {wordHighlight
                ? wordHighlight.passageWords.map((w, i) => (
                    <span key={i} style={{
                      color: wordHighlight.matched.has(i) ? T.green : T.cream,
                      background: wordHighlight.matched.has(i) ? "rgba(74,222,128,0.08)" : "transparent",
                      borderRadius: 3, padding: "1px 0",
                      transition: "color 0.2s",
                    }}>{w} </span>
                  ))
                : <span style={{ color: phase === "recording" ? T.cream : T.creamFaint }}>
                    "{passage}"
                  </span>
              }
            </div>

            {/* Live interim transcript */}
            {phase === "recording" && (interimText || transcript) && (
              <div style={{ marginTop: 16, padding: "12px 14px",
                background: "rgba(200,241,53,0.05)", borderRadius: 10,
                border: "1px solid rgba(200,241,53,0.15)" }}>
                <div style={{ fontSize: 10, color: T.creamFaint, marginBottom: 4,
                  textTransform: "uppercase", letterSpacing: 1 }}>Live Transcript</div>
                <div style={{ fontSize: 13, color: LIME, lineHeight: 1.7 }}>
                  {transcript}
                  <span style={{ color: "rgba(200,241,53,0.45)" }}>{interimText}</span>
                </div>
              </div>
            )}
          </DarkCard>

          {/* Recorder card */}
          <DarkCard style={{ padding: 28, textAlign: "center" }} hover={false}>
            <div style={{
              width: 90, height: 90, borderRadius: "50%", margin: "0 auto 16px",
              background: phase === "recording" ? "rgba(200,241,53,0.1)" : "rgba(255,255,255,0.04)",
              border: `2px solid ${phase === "recording" ? LIME : "rgba(255,255,255,0.1)"}`,
              display: "flex", alignItems: "center", justifyContent: "center", fontSize: 36,
              boxShadow: phase === "recording" ? `0 0 35px rgba(200,241,53,0.25)` : "none",
              animation: phase === "recording" ? "record-pulse 1.5s infinite" : "none",
            }}>
              {phase === "idle"        && "ğŸ™ï¸"}
              {phase === "recording"   && "ğŸŸ¢"}
              {phase === "processing"  && "â³"}
              {phase === "done"        && "âœ…"}
            </div>

            <div style={{ fontFamily: "'DM Sans',sans-serif", fontWeight: 900,
              fontSize: 44, color: T.cream, letterSpacing: 3, marginBottom: 6 }}>
              {fmt(timer)}
            </div>

            {/* Live stats during recording */}
            {phase === "recording" && (
              <div style={{ display: "flex", justifyContent: "center", gap: 20, marginBottom: 12 }}>
                {liveWpm > 0 && (
                  <div style={{ textAlign: "center" }}>
                    <div style={{ fontSize: 10, color: T.creamFaint, textTransform: "uppercase", letterSpacing: 1 }}>WPM</div>
                    <div style={{ fontWeight: 700, color: LIME, fontSize: 22 }}>{liveWpm}</div>
                  </div>
                )}
                <div style={{ textAlign: "center" }}>
                  <div style={{ fontSize: 10, color: T.creamFaint, textTransform: "uppercase", letterSpacing: 1 }}>Pauses</div>
                  <div style={{ fontWeight: 700, color: pauseRatio > 40 ? T.amber : T.green, fontSize: 22 }}>{pauseRatio}%</div>
                </div>
                {usingSpeechAPI && (
                  <div style={{ textAlign: "center" }}>
                    <div style={{ fontSize: 10, color: T.creamFaint, textTransform: "uppercase", letterSpacing: 1 }}>Words</div>
                    <div style={{ fontWeight: 700, color: T.blue, fontSize: 22 }}>
                      {tokenize(transcript).length}
                    </div>
                  </div>
                )}
              </div>
            )}

            <div style={{ color: T.creamFaint, fontSize: 13, marginBottom: 18 }}>
              {phase === "idle"       && "Tap Record to begin"}
              {phase === "recording"  && (usingSpeechAPI ? "Recording + transcribing liveâ€¦" : "Recording audioâ€¦")}
              {phase === "processing" && "Analysing speech featuresâ€¦"}
              {phase === "done"       && "âœ“ Speech analysis complete"}
            </div>

            <div style={{ display: "flex", gap: 10, justifyContent: "center" }}>
              {phase === "idle" && (
                <Btn onClick={startRec}>ğŸ™ï¸ Record</Btn>
              )}
              {phase === "recording" && (
                <>
                  <Btn onClick={() => stopRec(false)}>â¹ Stop & Analyse</Btn>
                  <Btn variant="ghost" onClick={() => stopRec(true)}>â†º Restart</Btn>
                </>
              )}
              {phase === "processing" && (
                <div style={{ color: T.creamFaint, fontSize: 13 }}>Please waitâ€¦</div>
              )}
            </div>

            {restartCount > 0 && (
              <div style={{ color: T.amber, fontSize: 12, marginTop: 10 }}>Restarts: {restartCount}</div>
            )}
          </DarkCard>
        </div>

        {/* â”€â”€ Right col: results â”€â”€ */}
        <DarkCard style={{ padding: 28 }} hover={false}>
          <div style={{ fontSize: 11, color: T.creamFaint, letterSpacing: 1,
            textTransform: "uppercase", marginBottom: 20 }}>Analysis Results</div>

          {phase === "done" && result ? (
            <>
              {/* Core metrics grid */}
              <div style={{ display: "grid", gridTemplateColumns: "1fr 1fr", gap: 10, marginBottom: 14 }}>
                {[
                  { label: "Speech Rate",    v: `${result.wpm} wpm`,
                    c: result.wpm > 90 && result.wpm < 190 ? T.green : T.amber,
                    tip: "90â€“190 wpm = normal" },
                  { label: "Pause Ratio",    v: `${Math.round(result.pauseRatio * 100)}%`,
                    c: result.pauseRatio < 0.3 ? T.green : result.pauseRatio < 0.5 ? T.amber : T.red,
                    tip: "< 30% = good fluency" },
                  { label: "Speed Var.",     v: `Â±${result.speedVar} wpm`,
                    c: result.speedVar < 25 ? T.green : T.amber,
                    tip: "Low variability = steady pace" },
                  { label: "Start Delay",    v: `${result.startDelay}s`,
                    c: result.startDelay < 1.5 ? T.green : T.amber,
                    tip: "Time to first word" },
                  { label: "Completion",     v: `${Math.round(result.compRatio * 100)}%`,
                    c: result.compRatio > 0.8 ? T.green : T.amber,
                    tip: "Fraction of passage read" },
                  { label: "Duration",       v: `${result.totalSec}s`,
                    c: T.cream, tip: "Total recording time" },
                ].map(m => (
                  <div key={m.label} style={{ background: T.bg3, borderRadius: 11, padding: "12px 14px" }}>
                    <div style={{ fontSize: 10, color: T.creamFaint, marginBottom: 3,
                      textTransform: "uppercase", letterSpacing: 0.5 }}>{m.label}</div>
                    <div style={{ fontWeight: 700, color: m.c, fontSize: 20 }}>{m.v}</div>
                    <div style={{ fontSize: 10, color: "#444", marginTop: 2 }}>{m.tip}</div>
                  </div>
                ))}
              </div>

              {/* AI transcription metrics (if available) */}
              {result.usedSpeechAPI && (
                <div style={{ background: "rgba(96,165,250,0.07)", borderRadius: 12, padding: 14,
                  border: "1px solid rgba(96,165,250,0.2)", marginBottom: 12 }}>
                  <div style={{ fontSize: 11, color: T.blue, fontWeight: 700, marginBottom: 10,
                    textTransform: "uppercase", letterSpacing: 1 }}>
                    ğŸ¤– AI Transcription Metrics
                  </div>
                  <div style={{ display: "grid", gridTemplateColumns: "1fr 1fr 1fr", gap: 8 }}>
                    {[
                      { label: "Word Accuracy",  v: result.accuracy != null ? `${result.accuracy}%` : "â€”",
                        c: result.accuracy > 80 ? T.green : result.accuracy > 60 ? T.amber : T.red },
                      { label: "Filler Words",   v: result.fillerCount,
                        c: result.fillerCount < 3 ? T.green : T.amber },
                      { label: "Repetitions",    v: result.repCount,
                        c: result.repCount < 2 ? T.green : T.amber },
                    ].map(m => (
                      <div key={m.label} style={{ textAlign: "center", background: T.bg3,
                        borderRadius: 9, padding: "10px 6px" }}>
                        <div style={{ fontSize: 9, color: T.creamFaint, textTransform: "uppercase",
                          letterSpacing: 0.5, marginBottom: 3 }}>{m.label}</div>
                        <div style={{ fontWeight: 700, color: m.c, fontSize: 18 }}>{m.v}</div>
                      </div>
                    ))}
                  </div>
                </div>
              )}

              {/* Transcript preview */}
              {result.usedSpeechAPI && transcript && (
                <div style={{ background: T.bg3, borderRadius: 10, padding: 12, marginBottom: 12,
                  maxHeight: 80, overflow: "hidden", position: "relative" }}>
                  <div style={{ fontSize: 10, color: T.creamFaint, marginBottom: 4,
                    textTransform: "uppercase", letterSpacing: 1 }}>Your Transcript</div>
                  <div style={{ fontSize: 12, color: T.creamDim, lineHeight: 1.6 }}>
                    {transcript.slice(0, 200)}{transcript.length > 200 ? "â€¦" : ""}
                  </div>
                </div>
              )}

              <div style={{ background: "rgba(74,222,128,0.08)", borderRadius: 12, padding: 14,
                border: "1px solid rgba(74,222,128,0.15)", marginBottom: 14 }}>
                <div style={{ fontWeight: 700, color: T.green, marginBottom: 4, fontSize: 13 }}>
                  âœ“ {result.usedSpeechAPI ? "8" : "5"} speech features extracted
                </div>
                <div style={{ color: T.creamFaint, fontSize: 12, lineHeight: 1.65 }}>
                  {result.usedSpeechAPI
                    ? "Real transcription: WPM, accuracy, pause ratio, variability, start delay, fillers, repetitions, completion."
                    : "Audio analysis: WPM, pause ratio, variability, start delay, completion. Enable Chrome for full AI transcription."}
                </div>
              </div>

              <Btn onClick={() => setPage("assessments")} style={{ width: "100%", justifyContent: "center" }}>
                â† Back to Tests
              </Btn>
            </>
          ) : (
            <div style={{ textAlign: "center", padding: "40px 0", color: T.creamFaint }}>
              <div style={{ fontSize: 44, marginBottom: 12, opacity: 0.2 }}>ğŸ“Š</div>
              <div style={{ fontSize: 14, lineHeight: 1.8, marginBottom: 16 }}>
                {phase === "idle"
                  ? "Results appear after you complete the recording."
                  : phase === "recording"
                  ? "Recording in progressâ€¦"
                  : "Analysingâ€¦"}
              </div>

              {/* Live WPM during recording (fallback mode) */}
              {phase === "recording" && !usingSpeechAPI && timer > 5 && (
                <div>
                  <div style={{ fontSize: 11, color: "#555", textTransform: "uppercase",
                    letterSpacing: 1, marginBottom: 4 }}>Est. WPM</div>
                  <div style={{ fontWeight: 900, fontSize: 38, color: LIME }}>
                    {Math.round((passageWords.length / timer) * 60)}
                  </div>
                </div>
              )}

              {/* Feature list preview */}
              {phase === "idle" && (
                <div style={{ marginTop: 16, textAlign: "left", display: "inline-block" }}>
                  {[
                    hasSpeechAPI ? "âœ“ AI transcription (Web Speech API)" : "âš  Audio mode (no transcription)",
                    "âœ“ Real-time pause detection (Web Audio)",
                    "âœ“ Word-level accuracy scoring",
                    "âœ“ Speech start delay measurement",
                    "âœ“ Speed variability per segment",
                    hasSpeechAPI ? "âœ“ Filler & repetition detection" : null,
                  ].filter(Boolean).map((f, i) => (
                    <div key={i} style={{ fontSize: 12, color: f.startsWith("âœ“") ? T.creamDim : T.amber,
                      marginBottom: 5 }}>{f}</div>
                  ))}
                </div>
              )}
            </div>
          )}
        </DarkCard>
      </div>
    </div>
  );
}
